"""
NIT_WARANGAL.ipynb

Automatically generated by Colab.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
import xgboost as xgb
import numpy as np
import matplotlib.pyplot as plt
#import seaborn as sns
from sklearn.utils import resample
import joblib

# Load the dataset
file_path = r"C:\Users\M Ravinder Rao\OneDrive\Desktop\flask\heart_failure_clinical_records_dataset.csv"
a = pd.read_csv(file_path)

import pandas as pd

a1 = pd.DataFrame(a)

a1=a1.dropna()
y=a1['DEATH_EVENT']

x=a1.drop(['DEATH_EVENT'],axis=1)

print('features size',x.shape)
print('target size',y.shape)

from imblearn.over_sampling import SMOTE

# Initializing SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)

# Applying SMOTE to the dataset
x,y= smote.fit_resample(x,y)
print(x.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.2, random_state=35)
#print(x_train)
#print(y_train)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the Logistic Regression model
model = LogisticRegression()

# Fit the model on the training data
model.fit(x_train, y_train)

# Predict on the test data
y_pred = model.predict(x_test)

# Evaluate the model
accuracy_LR = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy_LR)
#print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the Decision Tree classifier
model = DecisionTreeClassifier(random_state=42)

# Fit the model on the training data
model.fit(x_train, y_train)

# Predict on the test data
y_pred = model.predict(x_test)

# Evaluate the model
accuracy_DT = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy_DT)
#print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the Random Forest classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model on the training data
model.fit(x_train, y_train)

# Predict on the test data
y_pred = model.predict(x_test)

# Evaluate the model
accuracy_RF = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy_RF)
#print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

import xgboost as xgb
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the XGBClassifier
model = xgb.XGBClassifier(random_state=42)

# Fit the model on the training data
model.fit(x_train, y_train)

# Predict on the test data
y_pred = model.predict(x_test)

# Evaluate the model
accuracy_XG = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy_XG)
#print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score


# Create a confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Create a heatmap for the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix Heatmap')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.utils import resample
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt

# Number of bootstrap samples and iterations
n_bootstrap_samples = 50
n_iterations = 100

# Assuming x_test, y_test, x_train, y_train are already defined
model = LogisticRegression()
model.fit(x_test, y_test)

accuracy_scores = []

for i in range(n_iterations):
    # Resample with replacement from the training set
    x_resampled, y_resampled = resample(x_train, y_train, n_samples=n_bootstrap_samples)

    # Predict using the logistic regression model
    y_p = model.predict(x_resampled)

    # Calculate accuracy score
    accuracy = accuracy_score(y_resampled, y_p)
    accuracy_scores.append(accuracy)

# Find the iteration with the highest accuracy
max_accuracy = max(accuracy_scores)
best_iteration = accuracy_scores.index(max_accuracy)

# Plot the accuracy scores
plt.figure(figsize=(8, 6))
plt.plot(range(n_iterations), accuracy_scores)
plt.xlabel("Iteration")
plt.ylabel("Accuracy Score")
plt.title("Accuracy Scores for Bootstrapped Logistic Regression")
plt.show()

# Print the iteration and the highest accuracy
print(f"The highest accuracy is {max_accuracy:.4f} at iteration {best_iteration}.")

import matplotlib.pyplot as plt

algorithm_names = ['Logistic Regression','Decision Tree', 'Random Forest','Xgbost', 'Boostrap of Logistic Reggression']
accuracy_scores = [accuracy_LR,accuracy_DT, accuracy_RF,accuracy_XG,max_accuracy]

plt.figure(figsize=(10, 5))

# Bar chart for accuracy scores
plt.bar(algorithm_names, accuracy_scores, color='Green', alpha=0.7)
plt.xlabel('Classification Algorithms')
plt.ylabel('Accuracy Scores')
plt.title('Performance Comparison')

# Adding data labels
for i, v in enumerate(accuracy_scores):
    plt.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')

# Rotating the x-axis labels for better visibility
plt.xticks(rotation=45)

# Displaying the plot
plt.show()

algorithm_names = ['Logistic Regression','Decision Tree', 'Random Forest','Xgbost', 'Boostrap of Logistic Reggression']
accuracy_scores = [accuracy_LR,accuracy_DT, accuracy_RF,accuracy_XG,max_accuracy]
# Plotting the scores
plt.figure( figsize = ( 10 , 5 ) )
# Accuracy scores plot
plt.plot(algorithm_names, accuracy_scores, label = 'Accuracy', marker = 'o', color = '#99964A')
plt.xlabel( 'Classification Algorithms' )
plt.ylabel( 'Accuracy Scores' )
plt.title( 'Performance Comparison' )
# Adding a legend
plt.legend()
# Rotating the x-axis labels for better visibility
plt.xticks( rotation = 45 )
# Displaying the plot
plt.show()

#1 for anaemia, diabetes, high blood pressure, male,smoking habit presence
#1 stands for death
new_data = [[75,0,582,0,20,1,105000,1.9,130,1,0,4]]
predictions = model.predict(new_data)

# Print the predictions
print(predictions)

# Evaluate the model on the testing set
y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on the testing set: {accuracy}")

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(x_train, y_train)
print(grid_search.best_params_)

from sklearn.model_selection import cross_val_score

model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=None)
scores = cross_val_score(model, x, y, cv=5)
print(f"Cross-validation scores: {scores}")
print(f"Mean cross-validation score: {scores.mean()}")

model.fit(x_train, y_train)
importances = model.feature_importances_
feature_names = x_train.columns
sorted_indices = importances.argsort()
plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
plt.yticks(range(len(sorted_indices)), feature_names[sorted_indices])
plt.xlabel('Feature Importance')
plt.title('Feature Importances in Random Forest')
plt.show()

joblib.dump(model, 'model.pkl')
print("Model saved as model.pkl")
